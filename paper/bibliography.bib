% Encoding: UTF-8

@Article{Siu1991,
  author   = {Siu, Albert L.},
  journal  = {Annals of Internal Medicine},
  title    = {Screening for {Dementia} and {Investigating} {Its} {Causes}},
  year     = {1991},
  issn     = {0003-4819},
  month    = jul,
  number   = {2},
  pages    = {122},
  volume   = {115},
  doi      = {10.7326/0003-4819-115-2-122},
  file     = {:files/Siu1991.pdf:PDF},
  language = {en},
  url      = {http://annals.org/article.aspx?doi=10.7326/0003-4819-115-2-122},
  urldate  = {2021-05-24},
}

@InCollection{Barkhof2016,
  author    = {Barkhof, Frederik and van Buchem, Mark A.},
  booktitle = {Diseases of the {Brain}, {Head} and {Neck}, {Spine} 2016-2019},
  publisher = {Springer International Publishing},
  title     = {Neuroimaging in {Dementia}},
  year      = {2016},
  address   = {Cham},
  editor    = {Hodler, Jürg and Kubik-Huch, Rahel A. and von Schulthess, Gustav K.},
  isbn      = {978-3-319-30080-1 978-3-319-30081-8},
  pages     = {79--85},
  doi       = {10.1007/978-3-319-30081-8_10},
  file      = {:files/Barkhof2016.pdf:PDF},
  language  = {en},
  url       = {http://link.springer.com/10.1007/978-3-319-30081-8_10},
  urldate   = {2021-05-24},
}

@Article{Knopman2006,
  author    = {Knopman, David S. and Petersen, Ronald C. and Cha, Ruth H. and Edland, Steven D. and Rocca, Walter A.},
  journal   = {Archives of Neurology},
  title     = {Incidence and Causes of Nondegenerative Nonvascular Dementia},
  year      = {2006},
  month     = {feb},
  number    = {2},
  pages     = {218},
  volume    = {63},
  doi       = {10.1001/archneur.63.2.218},
  file      = {:files/Knopman2006.pdf:PDF},
  publisher = {American Medical Association ({AMA})},
}

@Article{Rocca2014,
  author    = {Rocca, Walter A. and Mielke, Michelle M. and Vemuri, Prashanthi and Miller, Virginia M.},
  journal   = {Maturitas},
  title     = {Sex and gender differences in the causes of dementia: A narrative review},
  year      = {2014},
  month     = {oct},
  number    = {2},
  pages     = {196--201},
  volume    = {79},
  doi       = {10.1016/j.maturitas.2014.05.008},
  file      = {:files/Rocca2014.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Zhou2019,
  author        = {Zhou, Yongjin and Huang, Weijian and Dong, Pei and Xia, Yong and Wang, Shanshan},
  journal       = {IEEE/ACM Transactions on Computational Biology and Bioinformatics (2019)},
  title         = {D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation},
  year          = {2019},
  month         = aug,
  abstract      = {Assessing the location and extent of lesions caused by chronic stroke is critical for medical diagnosis, surgical planning, and prognosis. In recent years, with the rapid development of 2D and 3D convolutional neural networks (CNN), the encoder-decoder structure has shown great potential in the field of medical image segmentation. However, the 2D CNN ignores the 3D information of medical images, while the 3D CNN suffers from high computational resource demands. This paper proposes a new architecture called dimension-fusion-UNet (D-UNet), which combines 2D and 3D convolution innovatively in the encoding stage. The proposed architecture achieves a better segmentation performance than 2D networks, while requiring significantly less computation time in comparison to 3D networks. Furthermore, to alleviate the data imbalance issue between positive and negative samples for the network training, we propose a new loss function called Enhance Mixing Loss (EML). This function adds a weighted focal coefficient and combines two traditional loss functions. The proposed method has been tested on the ATLAS dataset and compared to three state-of-the-art methods. The results demonstrate that the proposed method achieves the best quality performance in terms of DSC = 0.5349+0.2763 and precision = 0.6331+0.295).},
  archiveprefix = {arXiv},
  doi           = {10.1109/TCBB.2019.2939522.},
  eprint        = {1908.05104},
  file          = {:files/Zhou2019.pdf:PDF},
  keywords      = {eess.IV, cs.CV},
  primaryclass  = {eess.IV},
}

@Article{Dolz2018,
  author    = {Dolz, Jose and Desrosiers, Christian and Ayed, Ismail Ben},
  journal   = {{NeuroImage}},
  title     = {3D fully convolutional networks for subcortical segmentation in {MRI}: A large-scale study},
  year      = {2018},
  month     = {apr},
  pages     = {456--470},
  volume    = {170},
  doi       = {10.1016/j.neuroimage.2017.04.039},
  file      = {:files/Dolz2018.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Dalca2019,
  author        = {Dalca, Adrian V. and Yu, Evan and Golland, Polina and Fischl, Bruce and Sabuncu, Mert R. and Iglesias, Juan Eugenio},
  title         = {Unsupervised Deep Learning for Bayesian Brain MRI Segmentation},
  year          = {2019},
  month         = apr,
  abstract      = {Probabilistic atlas priors have been commonly used to derive adaptive and robust brain MRI segmentation algorithms. Widely-used neuroimage analysis pipelines rely heavily on these techniques, which are often computationally expensive. In contrast, there has been a recent surge of approaches that leverage deep learning to implement segmentation tools that are computationally efficient at test time. However, most of these strategies rely on learning from manually annotated images. These supervised deep learning methods are therefore sensitive to the intensity profiles in the training dataset. To develop a deep learning-based segmentation model for a new image dataset (e.g., of different contrast), one usually needs to create a new labeled training dataset, which can be prohibitively expensive, or rely on suboptimal ad hoc adaptation or augmentation approaches. In this paper, we propose an alternative strategy that combines a conventional probabilistic atlas-based segmentation with deep learning, enabling one to train a segmentation model for new MRI scans without the need for any manually segmented images. Our experiments include thousands of brain MRI scans and demonstrate that the proposed method achieves good accuracy for a brain MRI segmentation task for different MRI contrasts, requiring only approximately 15 seconds at test time on a GPU. The code is freely available at http://voxelmorph.mit.edu.},
  archiveprefix = {arXiv},
  eprint        = {1904.11319},
  file          = {:files/Dalca2019.pdf:PDF},
  keywords      = {cs.CV, eess.IV},
  primaryclass  = {cs.CV},
}

@Article{Bui2017,
  author        = {Bui, Toan Duc and Shin, Jitae and Moon, Taesup},
  title         = {3D Densely Convolutional Networks for Volumetric Segmentation},
  year          = {2017},
  month         = sep,
  abstract      = {In the isointense stage, the accurate volumetric image segmentation is a challenging task due to the low contrast between tissues. In this paper, we propose a novel very deep network architecture based on a densely convolutional network for volumetric brain segmentation. The proposed network architecture provides a dense connection between layers that aims to improve the information flow in the network. By concatenating features map of fine and coarse dense blocks, it allows capturing multi-scale contextual information. Experimental results demonstrate significant advantages of the proposed method over existing methods, in terms of both segmentation accuracy and parameter efficiency in MICCAI grand challenge on 6-month infant brain MRI segmentation.},
  archiveprefix = {arXiv},
  eprint        = {1709.03199},
  file          = {:files/Bui2017.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Buda2019,
  author        = {Buda, Mateusz and Saha, Ashirbani and Mazurowski, Maciej A.},
  journal       = {Computers in Biology and Medicine, 109, 2019, 218-225},
  title         = {Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm},
  year          = {2019},
  month         = jun,
  abstract      = {Recent analysis identified distinct genomic subtypes of lower-grade glioma tumors which are associated with shape features. In this study, we propose a fully automatic way to quantify tumor imaging characteristics using deep learning-based segmentation and test whether these characteristics are predictive of tumor genomic subtypes. We used preoperative imaging and genomic data of 110 patients from 5 institutions with lower-grade gliomas from The Cancer Genome Atlas. Based on automatic deep learning segmentations, we extracted three features which quantify two-dimensional and three-dimensional characteristics of the tumors. Genomic data for the analyzed cohort of patients consisted of previously identified genomic clusters based on IDH mutation and 1p/19q co-deletion, DNA methylation, gene expression, DNA copy number, and microRNA expression. To analyze the relationship between the imaging features and genomic clusters, we conducted the Fisher exact test for 10 hypotheses for each pair of imaging feature and genomic subtype. To account for multiple hypothesis testing, we applied a Bonferroni correction. P-values lower than 0.005 were considered statistically significant. We found the strongest association between RNASeq clusters and the bounding ellipsoid volume ratio ($p<0.0002$) and between RNASeq clusters and margin fluctuation ($p<0.005$). In addition, we identified associations between bounding ellipsoid volume ratio and all tested molecular subtypes ($p<0.02$) as well as between angular standard deviation and RNASeq cluster ($p<0.02$). In terms of automatic tumor segmentation that was used to generate the quantitative image characteristics, our deep learning algorithm achieved a mean Dice coefficient of 82% which is comparable to human performance.},
  archiveprefix = {arXiv},
  doi           = {10.1016/j.compbiomed.2019.05.002},
  eprint        = {1906.03720},
  file          = {:files/Buda2019.pdf:PDF},
  keywords      = {eess.IV, cs.CV, cs.LG},
  primaryclass  = {eess.IV},
}

@Article{Mondal2018,
  author        = {Mondal, Arnab Kumar and Dolz, Jose and Desrosiers, Christian},
  title         = {Few-shot 3D Multi-modal Medical Image Segmentation using Generative Adversarial Learning},
  year          = {2018},
  month         = oct,
  abstract      = {We address the problem of segmenting 3D multi-modal medical images in scenarios where very few labeled examples are available for training. Leveraging the recent success of adversarial learning for semi-supervised segmentation, we propose a novel method based on Generative Adversarial Networks (GANs) to train a segmentation model with both labeled and unlabeled images. The proposed method prevents over-fitting by learning to discriminate between true and fake patches obtained by a generator network. Our work extends current adversarial learning approaches, which focus on 2D single-modality images, to the more challenging context of 3D volumes of multiple modalities. The proposed method is evaluated on the problem of segmenting brain MRI from the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement is reported, compared to state-of-art segmentation networks trained in a fully-supervised manner. In addition, our work presents a comprehensive analysis of different GAN architectures for semi-supervised segmentation, showing recent techniques like feature matching to yield a higher performance than conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot_GAN-Unet3D},
  archiveprefix = {arXiv},
  eprint        = {1810.12241},
  file          = {:files/Mondal2018.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Rebsamen2020,
  author    = {Rebsamen, Michael and Rummel, Christian and Reyes, Mauricio and Wiest, Roland and McKinley, Richard},
  journal   = {Human Brain Mapping},
  title     = {Direct cortical thickness estimation using deep learning-based anatomy segmentation and cortex parcellation},
  year      = {2020},
  month     = {aug},
  number    = {17},
  pages     = {4804--4814},
  volume    = {41},
  doi       = {10.1002/hbm.25159},
  file      = {:files/Rebsamen2020.pdf:PDF},
  publisher = {Wiley},
}

@Article{Cerri2020,
  author        = {Stefano Cerri and Andrew Hoopes and Douglas N. Greve and Mark Mühlau and Koen Van Leemput},
  title         = {A Longitudinal Method for Simultaneous Whole-Brain and Lesion Segmentation in Multiple Sclerosis},
  year          = {2020},
  month         = aug,
  abstract      = {In this paper we propose a novel method for the segmentation of longitudinal brain MRI scans of patients suffering from Multiple Sclerosis. The method builds upon an existing cross-sectional method for simultaneous whole-brain and lesion segmentation, introducing subject-specific latent variables to encourage temporal consistency between longitudinal scans. It is very generally applicable, as it does not make any prior assumptions on the scanner, the MRI protocol, or the number and timing of longitudinal follow-up scans. Preliminary experiments on three longitudinal datasets indicate that the proposed method produces more reliable segmentations and detects disease effects better than the cross-sectional method it is based upon.},
  archiveprefix = {arXiv},
  doi           = {10.1007/978-3-030-66843-3_12},
  eprint        = {2008.05117},
  file          = {:files/Cerri2020.pdf:PDF},
  keywords      = {eess.IV, cs.CV, cs.LG},
  primaryclass  = {eess.IV},
}

@Article{Cerri2020a,
  author        = {Stefano Cerri and Oula Puonti and Dominik S. Meier and Jens Wuerfel and Mark Mühlau and Hartwig R. Siebner and Koen Van Leemput},
  title         = {A Contrast-Adaptive Method for Simultaneous Whole-Brain and Lesion Segmentation in Multiple Sclerosis},
  year          = {2020},
  month         = may,
  abstract      = {Here we present a method for the simultaneous segmentation of white matter lesions and normal-appearing neuroanatomical structures from multi-contrast brain MRI scans of multiple sclerosis patients. The method integrates a novel model for white matter lesions into a previously validated generative model for whole-brain segmentation. By using separate models for the shape of anatomical structures and their appearance in MRI, the algorithm can adapt to data acquired with different scanners and imaging protocols without retraining. We validate the method using four disparate datasets, showing robust performance in white matter lesion segmentation while simultaneously segmenting dozens of other brain structures. We further demonstrate that the contrast-adaptive method can also be safely applied to MRI scans of healthy controls, and replicate previously documented atrophy patterns in deep gray matter structures in MS. The algorithm is publicly available as part of the open-source neuroimaging package FreeSurfer.},
  archiveprefix = {arXiv},
  doi           = {10.1016/j.neuroimage.2020.117471},
  eprint        = {2005.05135},
  file          = {:files/Cerri2020a.pdf:PDF},
  keywords      = {eess.IV, cs.CV, cs.LG, q-bio.QM},
  primaryclass  = {eess.IV},
}

@Article{Rickmann2020,
  author        = {Anne-Marie Rickmann and Abhijit Guha Roy and Ignacio Sarasua and Christian Wachinger},
  title         = {Recalibrating 3D ConvNets with Project \& Excite},
  year          = {2020},
  month         = feb,
  abstract      = {Fully Convolutional Neural Networks (F-CNNs) achieve state-of-the-art performance for segmentation tasks in computer vision and medical imaging. Recently, computational blocks termed squeeze and excitation (SE) have been introduced to recalibrate F-CNN feature maps both channel- and spatial-wise, boosting segmentation performance while only minimally increasing the model complexity. So far, the development of SE blocks has focused on 2D architectures. For volumetric medical images, however, 3D F-CNNs are a natural choice. In this article, we extend existing 2D recalibration methods to 3D and propose a generic compress-process-recalibrate pipeline for easy comparison of such blocks. We further introduce Project \& Excite (PE) modules, customized for 3D networks. In contrast to existing modules, Project \& Excite does not perform global average pooling but compresses feature maps along different spatial dimensions of the tensor separately to retain more spatial information that is subsequently used in the excitation step. We evaluate the modules on two challenging tasks, whole-brain segmentation of MRI scans and whole-body segmentation of CT scans. We demonstrate that PE modules can be easily integrated into 3D F-CNNs, boosting performance up to 0.3 in Dice Score and outperforming 3D extensions of other recalibration blocks, while only marginally increasing the model complexity. Our code is publicly available on https://github.com/ai-med/squeeze_and_excitation .},
  archiveprefix = {arXiv},
  doi           = {10.1109/TMI.2020.2972059},
  eprint        = {2002.10994},
  file          = {:files/Rickmann2020.pdf:PDF},
  keywords      = {eess.IV, cs.LG, stat.ML},
  primaryclass  = {eess.IV},
}

@Article{Rickmann2019,
  author        = {Anne-Marie Rickmann and Abhijit Guha Roy and Ignacio Sarasua and Nassir Navab and Christian Wachinger},
  title         = {`Project \& Excite' Modules for Segmentation of Volumetric Medical Scans},
  year          = {2019},
  month         = jun,
  abstract      = {Fully Convolutional Neural Networks (F-CNNs) achieve state-of-the-art performance for image segmentation in medical imaging. Recently, squeeze and excitation (SE) modules and variations thereof have been introduced to recalibrate feature maps channel- and spatial-wise, which can boost performance while only minimally increasing model complexity. So far, the development of SE has focused on 2D images. In this paper, we propose `Project \& Excite' (PE) modules that base upon the ideas of SE and extend them to operating on 3D volumetric images. `Project \& Excite' does not perform global average pooling, but squeezes feature maps along different slices of a tensor separately to retain more spatial information that is subsequently used in the excitation step. We demonstrate that PE modules can be easily integrated in 3D U-Net, boosting performance by 5% Dice points, while only increasing the model complexity by 2%. We evaluate the PE module on two challenging tasks, whole-brain segmentation of MRI scans and whole-body segmentation of CT scans. Code: https://github.com/ai-med/squeeze_and_excitation},
  archiveprefix = {arXiv},
  eprint        = {1906.04649},
  file          = {:files/Rickmann2019.pdf:PDF},
  keywords      = {eess.IV, cs.CV},
  primaryclass  = {eess.IV},
}

@Article{Roy2018,
  author        = {Abhijit Guha Roy and Nassir Navab and Christian Wachinger},
  title         = {Concurrent Spatial and Channel Squeeze \& Excitation in Fully Convolutional Networks},
  year          = {2018},
  month         = mar,
  abstract      = {Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze \& excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze \& excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation on whole body contrast enhanced CT scans (Visceral Dataset).},
  archiveprefix = {arXiv},
  eprint        = {1803.02579},
  file          = {:files/Roy2018.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Dolz2017,
  author        = {Jose Dolz and Ismail Ben Ayed and Jing Yuan and Christian Desrosiers},
  title         = {Isointense Infant Brain Segmentation with a Hyper-dense Connected Convolutional Neural Network},
  year          = {2017},
  month         = oct,
  abstract      = {Neonatal brain segmentation in magnetic resonance (MR) is a challenging problem due to poor image quality and low contrast between white and gray matter regions. Most existing approaches for this problem are based on multi-atlas label fusion strategies, which are time-consuming and sensitive to registration errors. As alternative to these methods, we propose a hyper-densely connected 3D convolutional neural network that employs MR-T1 and T2 images as input, which are processed independently in two separated paths. An important difference with previous densely connected networks is the use of direct connections between layers from the same and different paths. Adopting such dense connectivity helps the learning process by including deep supervision and improving gradient flow. We evaluated our approach on data from the MICCAI Grand Challenge on 6-month infant Brain MRI Segmentation (iSEG), obtaining very competitive results. Among 21 teams, our approach ranked first or second in most metrics, translating into a state-of-the-art performance.},
  archiveprefix = {arXiv},
  eprint        = {1710.05956},
  file          = {:files/Dolz2017.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Roy2018a,
  author        = {Abhijit Guha Roy and Sailesh Conjeti and Nassir Navab and Christian Wachinger},
  title         = {Bayesian QuickNAT: Model Uncertainty in Deep Whole-Brain Segmentation for Structure-wise Quality Control},
  year          = {2018},
  month         = nov,
  abstract      = {We introduce Bayesian QuickNAT for the automated quality control of whole-brain segmentation on MRI T1 scans. Next to the Bayesian fully convolutional neural network, we also present inherent measures of segmentation uncertainty that allow for quality control per brain structure. For estimating model uncertainty, we follow a Bayesian approach, wherein, Monte Carlo (MC) samples from the posterior distribution are generated by keeping the dropout layers active at test time. Entropy over the MC samples provides a voxel-wise model uncertainty map, whereas expectation over the MC predictions provides the final segmentation. Next to voxel-wise uncertainty, we introduce four metrics to quantify structure-wise uncertainty in segmentation for quality control. We report experiments on four out-of-sample datasets comprising of diverse age range, pathology and imaging artifacts. The proposed structure-wise uncertainty metrics are highly correlated with the Dice score estimated with manual annotation and therefore present an inherent measure of segmentation quality. In particular, the intersection over union over all the MC samples is a suitable proxy for the Dice score. In addition to quality control at scan-level, we propose to incorporate the structure-wise uncertainty as a measure of confidence to do reliable group analysis on large data repositories. We envisage that the introduced uncertainty metrics would help assess the fidelity of automated deep learning based segmentation methods for large-scale population studies, as they enable automated quality control and group analyses in processing large data repositories.},
  archiveprefix = {arXiv},
  eprint        = {1811.09800},
  file          = {:files/Roy2018a.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Roy2018b,
  author        = {Abhijit Guha Roy and Sailesh Conjeti and Nassir Navab and Christian Wachinger},
  title         = {QuickNAT: A Fully Convolutional Network for Quick and Accurate Segmentation of Neuroanatomy},
  year          = {2018},
  month         = jan,
  abstract      = {Whole brain segmentation from structural magnetic resonance imaging (MRI) is a prerequisite for most morphological analyses, but is computationally intense and can therefore delay the availability of image markers after scan acquisition. We introduce QuickNAT, a fully convolutional, densely connected neural network that segments a \revision{MRI brain scan} in 20 seconds. To enable training of the complex network with millions of learnable parameters using limited annotated data, we propose to first pre-train on auxiliary labels created from existing segmentation software. Subsequently, the pre-trained model is fine-tuned on manual labels to rectify errors in auxiliary labels. With this learning strategy, we are able to use large neuroimaging repositories without manual annotations for training. In an extensive set of evaluations on eight datasets that cover a wide age range, pathology, and different scanners, we demonstrate that QuickNAT achieves superior segmentation accuracy and reliability in comparison to state-of-the-art methods, while being orders of magnitude faster. The speed up facilitates processing of large data repositories and supports translation of imaging biomarkers by making them available within seconds for fast clinical decision making.},
  archiveprefix = {arXiv},
  eprint        = {1801.04161},
  file          = {:files/Roy2018b.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Huo2019,
  author        = {Yuankai Huo and Zhoubing Xu and Yunxi Xiong and Katherine Aboud and Prasanna Parvathaneni and Shunxing Bao and Camilo Bermudez and Susan M. Resnick and Laurie E. Cutting and Bennett A. Landman},
  title         = {3D Whole Brain Segmentation using Spatially Localized Atlas Network Tiles},
  year          = {2019},
  month         = mar,
  abstract      = {Detailed whole brain segmentation is an essential quantitative technique, which provides a non-invasive way of measuring brain regions from a structural magnetic resonance imaging (MRI). Recently, deep convolution neural network (CNN) has been applied to whole brain segmentation. However, restricted by current GPU memory, 2D based methods, downsampling based 3D CNN methods, and patch-based high-resolution 3D CNN methods have been the de facto standard solutions. 3D patch-based high resolution methods typically yield superior performance among CNN approaches on detailed whole brain segmentation (>100 labels), however, whose performance are still commonly inferior compared with multi-atlas segmentation methods (MAS) due to the following challenges: (1) a single network is typically used to learn both spatial and contextual information for the patches, (2) limited manually traced whole brain volumes are available (typically less than 50) for training a network. In this work, we propose the spatially localized atlas network tiles (SLANT) method to distribute multiple independent 3D fully convolutional networks (FCN) for high-resolution whole brain segmentation. To address the first challenge, multiple spatially distributed networks were used in the SLANT method, in which each network learned contextual information for a fixed spatial location. To address the second challenge, auxiliary labels on 5111 initially unlabeled scans were created by multi-atlas segmentation for training. Since the method integrated multiple traditional medical image processing methods with deep learning, we developed a containerized pipeline to deploy the end-to-end solution. From the results, the proposed method achieved superior performance compared with multi-atlas segmentation methods, while reducing the computational time from >30 hours to 15 minutes (https://github.com/MASILab/SLANTbrainSeg).},
  archiveprefix = {arXiv},
  eprint        = {1903.12152},
  file          = {:files/Huo2019.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Billot2020,
  author        = {Benjamin Billot and Douglas Greve and Koen Van Leemput and Bruce Fischl and Juan Eugenio Iglesias and Adrian V. Dalca},
  journal       = {Proceedings of the Third Conference on Medical Imaging with Deep Learning (2020), vol.121, pp. 75-93},
  title         = {A Learning Strategy for Contrast-agnostic MRI Segmentation},
  year          = {2020},
  month         = mar,
  abstract      = {We present a deep learning strategy that enables, for the first time, contrast-agnostic semantic segmentation of completely unpreprocessed brain MRI scans, without requiring additional training or fine-tuning for new modalities. Classical Bayesian methods address this segmentation problem with unsupervised intensity models, but require significant computational resources. In contrast, learning-based methods can be fast at test time, but are sensitive to the data available at training. Our proposed learning method, SynthSeg, leverages a set of training segmentations (no intensity images required) to generate synthetic sample images of widely varying contrasts on the fly during training. These samples are produced using the generative model of the classical Bayesian segmentation framework, with randomly sampled parameters for appearance, deformation, noise, and bias field. Because each mini-batch has a different synthetic contrast, the final network is not biased towards any MRI contrast. We comprehensively evaluate our approach on four datasets comprising over 1,000 subjects and four types of MR contrast. The results show that our approach successfully segments every contrast in the data, performing slightly better than classical Bayesian segmentation, and three orders of magnitude faster. Moreover, even within the same type of MRI contrast, our strategy generalizes significantly better across datasets, compared to training using real images. Finally, we find that synthesizing a broad range of contrasts, even if unrealistic, increases the generalization of the neural network. Our code and model are open source at https://github.com/BBillot/SynthSeg.},
  archiveprefix = {arXiv},
  eprint        = {2003.01995},
  file          = {:files/Billot2020.pdf:PDF},
  keywords      = {eess.IV, cs.CV},
  primaryclass  = {eess.IV},
}

@Article{Huo2018,
  author        = {Yuankai Huo and Zhoubing Xu and Katherine Aboud and Prasanna Parvathaneni and Shunxing Bao and Camilo Bermudez and Susan M. Resnick and Laurie E. Cutting and Bennett A. Landman},
  title         = {Spatially Localized Atlas Network Tiles Enables 3D Whole Brain Segmentation from Limited Data},
  year          = {2018},
  month         = jun,
  abstract      = {Whole brain segmentation on a structural magnetic resonance imaging (MRI) is essential in non-invasive investigation for neuroanatomy. Historically, multi-atlas segmentation (MAS) has been regarded as the de facto standard method for whole brain segmentation. Recently, deep neural network approaches have been applied to whole brain segmentation by learning random patches or 2D slices. Yet, few previous efforts have been made on detailed whole brain segmentation using 3D networks due to the following challenges: (1) fitting entire whole brain volume into 3D networks is restricted by the current GPU memory, and (2) the large number of targeting labels (e.g., > 100 labels) with limited number of training 3D volumes (e.g., < 50 scans). In this paper, we propose the spatially localized atlas network tiles (SLANT) method to distribute multiple independent 3D fully convolutional networks to cover overlapped sub-spaces in a standard atlas space. This strategy simplifies the whole brain learning task to localized sub-tasks, which was enabled by combing canonical registration and label fusion techniques with deep learning. To address the second challenge, auxiliary labels on 5111 initially unlabeled scans were created by MAS for pre-training. From empirical validation, the state-of-the-art MAS method achieved mean Dice value of 0.76, 0.71, and 0.68, while the proposed method achieved 0.78, 0.73, and 0.71 on three validation cohorts. Moreover, the computational time reduced from > 30 hours using MAS to ~15 minutes using the proposed method. The source code is available online https://github.com/MASILab/SLANTbrainSeg},
  archiveprefix = {arXiv},
  eprint        = {1806.00546},
  file          = {:files/Huo2018.pdf:PDF},
  keywords      = {cs.CV, q-bio.NC},
  primaryclass  = {cs.CV},
}

@Article{McClure2018,
  author        = {Patrick McClure and Nao Rho and John A. Lee and Jakub R. Kaczmarzyk and Charles Zheng and Satrajit S. Ghosh and Dylan Nielson and Adam G. Thomas and Peter Bandettini and Francisco Pereira},
  title         = {Knowing what you know in brain segmentation using Bayesian deep neural networks},
  year          = {2018},
  month         = dec,
  abstract      = {In this paper, we describe a Bayesian deep neural network (DNN) for predicting FreeSurfer segmentations of structural MRI volumes, in minutes rather than hours. The network was trained and evaluated on a large dataset (n = 11,480), obtained by combining data from more than a hundred different sites, and also evaluated on another completely held-out dataset (n = 418). The network was trained using a novel spike-and-slab dropout-based variational inference approach. We show that, on these datasets, the proposed Bayesian DNN outperforms previously proposed methods, in terms of the similarity between the segmentation predictions and the FreeSurfer labels, and the usefulness of the estimate uncertainty of these predictions. In particular, we demonstrated that the prediction uncertainty of this network at each voxel is a good indicator of whether the network has made an error and that the uncertainty across the whole brain can predict the manual quality control ratings of a scan. The proposed Bayesian DNN method should be applicable to any new network architecture for addressing the segmentation problem.},
  archiveprefix = {arXiv},
  eprint        = {1812.01719},
  file          = {:files/McClure2018.pdf:PDF},
  keywords      = {cs.CV, cs.LG, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{Brudfors2018,
  author        = {Mikael Brudfors and Yael Balbastre and Parashkev Nachev and John Ashburner},
  journal       = {MIUA 2018. Communications in Computer and Information Science, vol 894},
  title         = {MRI Super-Resolution using Multi-Channel Total Variation},
  year          = {2018},
  month         = oct,
  abstract      = {This paper presents a generative model for super-resolution in routine clinical magnetic resonance images (MRI), of arbitrary orientation and contrast. The model recasts the recovery of high resolution images as an inverse problem, in which a forward model simulates the slice-select profile of the MR scanner. The paper introduces a prior based on multi-channel total variation for MRI super-resolution. Bias-variance trade-off is handled by estimating hyper-parameters from the low resolution input scans. The model was validated on a large database of brain images. The validation showed that the model can improve brain segmentation, that it can recover anatomical information between images of different MR contrasts, and that it generalises well to the large variability present in MR images of different subjects. The implementation is freely available at https://github.com/brudfors/spm_superres},
  archiveprefix = {arXiv},
  doi           = {10.1007/978-3-030-00928-1_97},
  eprint        = {1810.03422},
  file          = {:files/Brudfors2018.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Bernal2018,
  author        = {Jose Bernal and Kaisar Kushibar and Mariano Cabezas and Sergi Valverde and Arnau Oliver and Xavier Lladó},
  title         = {Quantitative analysis of patch-based fully convolutional neural networks for tissue segmentation on brain magnetic resonance imaging},
  year          = {2018},
  month         = jan,
  abstract      = {Accurate brain tissue segmentation in Magnetic Resonance Imaging (MRI) has attracted the attention of medical doctors and researchers since variations in tissue volume help in diagnosing and monitoring neurological diseases. Several proposals have been designed throughout the years comprising conventional machine learning strategies as well as convolutional neural networks (CNN) approaches. In particular, in this paper, we analyse a sub-group of deep learning methods producing dense predictions. This branch, referred in the literature as Fully CNN (FCNN), is of interest as these architectures can process an input volume in less time than CNNs and local spatial dependencies may be encoded since several voxels are classified at once. Our study focuses on understanding architectural strengths and weaknesses of literature-like approaches. Hence, we implement eight FCNN architectures inspired by robust state-of-the-art methods on brain segmentation related tasks. We evaluate them using the IBSR18, MICCAI2012 and iSeg2017 datasets as they contain infant and adult data and exhibit varied voxel spacing, image quality, number of scans and available imaging modalities. The discussion is driven in three directions: comparison between 2D and 3D approaches, the importance of multiple modalities and overlapping as a sampling strategy for training and testing models. To encourage other researchers to explore the evaluation framework, a public version is accessible to download from our research website.},
  archiveprefix = {arXiv},
  doi           = {10.1109/ACCESS.2019.2926697},
  eprint        = {1801.06457},
  file          = {:files/Bernal2018.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Bdair2020,
  author        = {Tariq Bdair and Benedikt Wiestler and Nassir Navab and Shadi Albarqouni},
  title         = {ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging},
  year          = {2020},
  month         = mar,
  abstract      = {Medical image segmentation is one of the major challenges addressed by machine learning methods. Yet, deep learning methods profoundly depend on a large amount of annotated data, which is time-consuming and costly. Though, semi-supervised learning methods approach this problem by leveraging an abundant amount of unlabeled data along with a small amount of labeled data in the training process. Recently, MixUp regularizer has been successfully introduced to semi-supervised learning methods showing superior performance. MixUp augments the model with new data points through linear interpolation of the data at the input space. We argue that this option is limited. Instead, we propose ROAM, a RandOm lAyer Mixup, which encourages the network to be less confident for interpolated data points at randomly selected space. ROAM generates more data points that have never seen before, and hence it avoids over-fitting and enhances the generalization ability. We conduct extensive experiments to validate our method on three publicly available datasets on whole-brain image segmentation. ROAM achieves state-of-the-art (SOTA) results in fully supervised (89.5%) and semi-supervised (87.0%) settings with a relative improvement of up to 2.40% and 16.50%, respectively for the whole-brain segmentation.},
  archiveprefix = {arXiv},
  eprint        = {2003.09439},
  file          = {:files/Bdair2020.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Brebisson2015,
  author        = {Alexandre de Brebisson and Giovanni Montana},
  title         = {Deep Neural Networks for Anatomical Brain Segmentation},
  year          = {2015},
  month         = feb,
  abstract      = {We present a novel approach to automatically segment magnetic resonance (MR) images of the human brain into anatomical regions. Our methodology is based on a deep artificial neural network that assigns each voxel in an MR image of the brain to its corresponding anatomical region. The inputs of the network capture information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches capture the local spatial context while large, compressed 2D orthogonal patches and distances to the regional centroids enforce global spatial consistency. Contrary to commonly used segmentation methods, our technique does not require any non-linear registration of the MR images. To benchmark our model, we used the dataset provided for the MICCAI 2012 challenge on multi-atlas labelling, which consists of 35 manually segmented MR images of the brain. We obtained competitive results (mean dice coefficient 0.725, error rate 0.163) showing the potential of our approach. To our knowledge, our technique is the first to tackle the anatomical segmentation of the whole brain using deep neural networks.},
  archiveprefix = {arXiv},
  eprint        = {1502.02445},
  file          = {:files/Brebisson2015.pdf:PDF},
  keywords      = {cs.CV, cs.LG, stat.AP, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{Berr2005,
  author    = {Berr, C. and Wancata, J. and Ritchie, K.},
  journal   = {European Neuropsychopharmacology},
  title     = {Prevalence of Dementia in the Elderly in Europe},
  year      = {2005},
  month     = {aug},
  number    = {4},
  pages     = {463--471},
  volume    = {15},
  doi       = {10.1016/j.euroneuro.2005.04.003},
  publisher = {Elsevier {BV}},
}

@Article{Speechly2008,
  author    = {Speechly, Catherine M. and Bridges-Webb, Charles and Passmore, Erin},
  journal   = {Medical Journal of Australia},
  title     = {The pathway to dementia diagnosis},
  year      = {2008},
  month     = {nov},
  number    = {9},
  pages     = {487--489},
  volume    = {189},
  doi       = {10.5694/j.1326-5377.2008.tb02140.x},
  file      = {:/home/andrea/Documents/UniDrive/UniBa/Computer Science/Projects/Computer Vision/files/Speechly2008.pdf:PDF},
  publisher = {{AMPCo}},
}

@Article{Fiske2005,
  author    = {Fiske, Amy and Gatz, Margaret and Aadn{\o}y, Britt and Pedersen, Nancy L.},
  journal   = {Alzheimer Disease {\&} Associated Disorders},
  title     = {Assessing Age of Dementia Onset},
  year      = {2005},
  month     = {jul},
  number    = {3},
  pages     = {128--134},
  volume    = {19},
  doi       = {10.1097/01.wad.0000174947.76968.74},
  file      = {:/home/andrea/Documents/UniDrive/UniBa/Computer Science/Projects/Computer Vision/files/Fiske2005.pdf:PDF},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
}

@Article{LaMontagne2019,
  author    = {LaMontagne, Pamela J. and Benzinger, Tammie L. S. and Morris, John C. and Keefe, Sarah and Hornbeck, Russ and Xiong, Chengjie and Grant, Elizabeth and Hassenstab, Jason and Moulder, Krista and Vlassenko, Andrei G. and Raichle, Marcus E. and Cruchaga, Carlos and Marcus, Daniel},
  title     = {{OASIS}-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease},
  year      = {2019},
  month     = {dec},
  doi       = {10.1101/2019.12.13.19014902},
  publisher = {Cold Spring Harbor Laboratory},
}

@Article{Klunk2004,
  author    = {Klunk, William E. and Engler, Henry and Nordberg, Agneta and Wang, Yanming and Blomqvist, Gunnar and Holt, Daniel P. and Bergström, Mats and Savitcheva, Irina and Huang, Guo-Feng and Estrada, Sergio and Aus{\'{e}}n, Birgitta and Debnath, Manik L. and Barletta, Julien and Price, Julie C. and Sandell, Johan and Lopresti, Brian J. and Wall, Anders and Koivisto, Pernilla and Antoni, Gunnar and Mathis, Chester A. and L{\aa}ngström, Bengt},
  journal   = {Annals of Neurology},
  title     = {Imaging brain amyloid in Alzheimer's disease with Pittsburgh Compound-B},
  year      = {2004},
  month     = {jan},
  number    = {3},
  pages     = {306--319},
  volume    = {55},
  doi       = {10.1002/ana.20009},
  publisher = {Wiley},
}

@Article{Camus2012,
  author    = {Camus, V. and Payoux, P. and Barr{\'{e}}, L. and Desgranges, B. and Voisin, T. and Tauber, C. and Joie, R. La and Tafani, M. and Hommet, C. and Ch{\'{e}}telat, G. and Mondon, K. and de La Sayette, V. and Cottier, J. P. and Beaufils, E. and Ribeiro, M. J. and Gissot, V. and Vierron, E. and Vercouillie, J. and Vellas, B. and Eustache, F. and Guilloteau, D.},
  journal   = {European Journal of Nuclear Medicine and Molecular Imaging},
  title     = {Using {PET} with 18F-{AV}-45 (florbetapir) to quantify brain amyloid load in a clinical environment},
  year      = {2012},
  month     = {jan},
  number    = {4},
  pages     = {621--631},
  volume    = {39},
  doi       = {10.1007/s00259-011-2021-8},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Ewers2013,
  author    = {M. Ewers and P. S. Insel and Y. Stern and M. W. Weiner and},
  journal   = {Neurology},
  title     = {Cognitive reserve associated with {FDG}-{PET} in preclinical Alzheimer disease},
  year      = {2013},
  month     = {mar},
  number    = {13},
  pages     = {1194--1201},
  volume    = {80},
  doi       = {10.1212/wnl.0b013e31828970c2},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
}

@Article{Newberg2002,
  author    = {Newberg, Andrew and Alavi, Abass and Reivich, Martin},
  journal   = {Seminars in Nuclear Medicine},
  title     = {Determination of regional cerebral function with {FDG}-{PET} imaging in neuropsychiatric disorders},
  year      = {2002},
  month     = {jan},
  number    = {1},
  pages     = {13--34},
  volume    = {32},
  doi       = {10.1053/snuc.2002.29276},
  publisher = {Elsevier {BV}},
}

@Article{Chetelat2020,
  author    = {Ch{\'{e}}telat, Ga{\"e}l and Arbizu, Javier and Barthel, Henryk and Garibotto, Valentina and Law, Ian and Morbelli, Silvia and van de Giessen, Elsmarieke and Agosta, Federica and Barkhof, Frederik and Brooks, David J. and Carrillo, Maria C. and Dubois, Bruno and Fjell, Anders M. and Frisoni, Giovanni B. and Hansson, Oskar and Herholz, Karl and Hutton, Brian F. and Jack, Clifford R. and Lammertsma, Adriaan A. and Landau, Susan M. and Minoshima, Satoshi and Nobili, Flavio and Nordberg, Agneta and Ossenkoppele, Rik and Oyen, Wim J. G. and Perani, Daniela and Rabinovici, Gil D. and Scheltens, Philip and Villemagne, Victor L. and Zetterberg, Henrik and Drzezga, Alexander},
  journal   = {The Lancet Neurology},
  title     = {Amyloid-{PET} and 18F-{FDG}-{PET} in the diagnostic investigation of Alzheimer's disease and other dementias},
  year      = {2020},
  month     = {nov},
  number    = {11},
  pages     = {951--962},
  volume    = {19},
  doi       = {10.1016/s1474-4422(20)30314-8},
  publisher = {Elsevier {BV}},
}

@Article{Morris1997,
  author    = {Morris, John C.},
  journal   = {International Psychogeriatrics},
  title     = {Clinical Dementia Rating: A Reliable and Valid Diagnostic and Staging Measure for Dementia of the Alzheimer Type},
  year      = {1997},
  month     = {dec},
  number    = {S1},
  pages     = {173--176},
  volume    = {9},
  doi       = {10.1017/s1041610297004870},
  publisher = {Cambridge University Press ({CUP})},
}

@Software{Pandas-v1.2.4,
  author         = {Reback, Jeff and McKinney, Wes and jbrockmendel and den Bossche, Joris Van and Augspurger, Tom and Cloud, Phillip and Hawkins, Simon and gfyoung and Sinhrks and Roeschke, Matthew and Klein, Adam and Petersen, Terji and Tratner, Jeff and She, Chang and Ayd, William and Naveh, Shahar and patrick and Garcia, Marc and Schendel, Jeremy and Hayden, Andy and Saxton, Daniel and Jancauskas, Vytautas and Gorelli, Marco and Shadrach, Richard and McMaster, Ali and Battiston, Pietro and Seabold, Skipper and Dong, Kaiqi and chris-b1 and h-vetinari},
  doi            = {10.5281/zenodo.4681666},
  month          = apr,
  publisher      = {Zenodo},
  qualityassured = {qualityAssured},
  title          = {pandas-dev/pandas: Pandas 1.2.4},
  url            = {https://doi.org/10.5281/zenodo.4681666},
  version        = {v1.2.4},
  year           = {2021},
}

@Article{Japkowicz2002,
  author    = {Japkowicz, Nathalie and Stephen, Shaju},
  journal   = {Intelligent Data Analysis},
  title     = {The class imbalance problem: A systematic study},
  year      = {2002},
  issn      = {15714128, 1088467X},
  pages     = {429-449},
  volume    = {6},
  doi       = {10.3233/IDA-2002-6504},
  file      = {:/Volumes/GoogleDrive/My Drive/UniBa/Computer Science/Projects/Computer Vision/files/Japkowicz2002.pdf:PDF},
  publisher = {IOS Press},
}

@Article{Ando2017,
  author        = {Ando, Shin and Huang, Chun-Yuan},
  title         = {Deep Over-sampling Framework for Classifying Imbalanced Data},
  year          = {2017},
  month         = apr,
  abstract      = {Class imbalance is a challenging issue in practical classification problems for deep learning models as well as traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this paper, we propose Deep Over-sampling (DOS), a framework for extending the synthetic over-sampling method to exploit the deep feature space acquired by a convolutional neural network (CNN). Its key feature is an explicit, supervised representation learning, for which the training data presents each raw input sample with a synthetic embedding target in the deep feature space, which is sampled from the linear subspace of in-class neighbors. We implement an iterative process of training the CNN and updating the targets, which induces smaller in-class variance among the embeddings, to increase the discriminative power of the deep representation. We present an empirical study using public benchmarks, which shows that the DOS framework not only counteracts class imbalance better than the existing method, but also improves the performance of the CNN in the standard, balanced settings.},
  archiveprefix = {arXiv},
  eprint        = {1704.07515},
  file          = {:http\://arxiv.org/pdf/1704.07515v3:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Zhou2020,
  author    = {Zhou, Ding-Xuan},
  journal   = {Neural Networks},
  title     = {Theory of deep convolutional neural networks: Downsampling},
  year      = {2020},
  month     = {apr},
  pages     = {319--327},
  volume    = {124},
  doi       = {10.1016/j.neunet.2020.01.018},
  file      = {:https\://www.cityu.edu.hk/rcms/pdf/XDZhou/dxZhou2020b.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Altay2020,
  author        = {Altay, Fatih and Sanchez, Guillermo Ramon and James, Yanli and Faraone, Stephen V. and Velipasalar, Senem and Salekin, Asif},
  title         = {Preclinical Stage Alzheimer's Disease Detection Using Magnetic Resonance Image Scans},
  year          = {2020},
  month         = nov,
  abstract      = {Alzheimer's disease is one of the diseases that mostly affects older people without being a part of aging. The most common symptoms include problems with communicating and abstract thinking, as well as disorientation. It is important to detect Alzheimer's disease in early stages so that cognitive functioning would be improved by medication and training. In this paper, we propose two attention model networks for detecting Alzheimer's disease from MRI images to help early detection efforts at the preclinical stage. We also compare the performance of these two attention network models with a baseline model. Recently available OASIS-3 Longitudinal Neuroimaging, Clinical, and Cognitive Dataset is used to train, evaluate and compare our models. The novelty of this research resides in the fact that we aim to detect Alzheimer's disease when all the parameters, physical assessments, and clinical data state that the patient is healthy and showing no symptoms},
  archiveprefix = {arXiv},
  eprint        = {2011.14139},
  file          = {:http\://arxiv.org/pdf/2011.14139v1:PDF},
  keywords      = {eess.IV, cs.LG},
  primaryclass  = {eess.IV},
}

@Article{Dosovitskiy2020,
  author        = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year          = {2020},
  month         = oct,
  abstract      = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  eprint        = {2010.11929},
  file          = {:http\://arxiv.org/pdf/2010.11929v2:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Otsu1979,
  author    = {Otsu, Nobuyuki},
  journal   = {{IEEE} Transactions on Systems, Man, and Cybernetics},
  title     = {A Threshold Selection Method from Gray-Level Histograms},
  year      = {1979},
  month     = {jan},
  number    = {1},
  pages     = {62--66},
  volume    = {9},
  doi       = {10.1109/tsmc.1979.4310076},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Zunair2020,
  author        = {Zunair, Hasib and Rahman, Aimon and Mohammed, Nabeel and Cohen, Joseph Paul},
  title         = {Uniformizing Techniques to Process {CT} scans with {3D} {CNNs} for Tuberculosis Prediction},
  year          = {2020},
  month         = jul,
  abstract      = {A common approach to medical image analysis on volumetric data uses deep 2D convolutional neural networks (CNNs). This is largely attributed to the challenges imposed by the nature of the 3D data: variable volume size, GPU exhaustion during optimization. However, dealing with the individual slices independently in 2D CNNs deliberately discards the depth information which results in poor performance for the intended task. Therefore, it is important to develop methods that not only overcome the heavy memory and computation requirements but also leverage the 3D information. To this end, we evaluate a set of volume uniformizing methods to address the aforementioned issues. The first method involves sampling information evenly from a subset of the volume. Another method exploits the full geometry of the 3D volume by interpolating over the z-axis. We demonstrate performance improvements using controlled ablation studies as well as put this approach to the test on the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark. We report 73% area under curve (AUC) and binary classification accuracy (ACC) of 67.5% on the test set beating all methods which leveraged only image information (without using clinical meta-data) achieving 5-th position overall. All codes and models are made available at https://github.com/hasibzunair/uniformizing-3D.},
  archiveprefix = {arXiv},
  eprint        = {2007.13224},
  file          = {:http\://arxiv.org/pdf/2007.13224v1:PDF},
  keywords      = {eess.IV, cs.CV},
  primaryclass  = {eess.IV},
}

@Misc{Zunair2020Implementation,
  author       = {Zunair, Hasib},
  howpublished = {\url{https://keras.io/examples/vision/3D_image_classification/}},
  month        = sep,
  note         = {Accessed: 2021-06-29},
  title        = {3D image classification from CT scans},
  year         = {2020},
  abstract     = {Train a 3D convolutional neural network to predict presence of pneumonia.},
  language     = {en},
  url          = {https://keras.io/examples/vision/3D_image_classification/},
}

@Article{Hiltunen2009,
  author    = {Hiltunen, Mikko and van Groen, Thomas and Jolkkonen, Jukka},
  journal   = {Journal of Alzheimer's Disease},
  title     = {Functional Roles of Amyloid-$\beta$ Protein Precursor and Amyloid-$\beta$ Peptides: Evidence from Experimental Studies},
  year      = {2009},
  issn      = {18758908, 13872877},
  pages     = {401-412},
  volume    = {18},
  doi       = {10.3233/JAD-2009-1154},
  publisher = {IOS Press},
}

@Article{Rabinovici2011,
  author    = {G. D. Rabinovici and H. J. Rosen and A. Alkalay and J. Kornak and A. J. Furst and N. Agarwal and E. C. Mormino and J. P. O'Neil and M. Janabi and A. Karydas and M. E. Growdon and J. Y. Jang and E. J. Huang and S. J. DeArmond and J. Q. Trojanowski and L. T. Grinberg and M. L. Gorno-Tempini and W. W. Seeley and B. L. Miller and W. J. Jagust},
  journal   = {Neurology},
  title     = {Amyloid vs {FDG}-{PET} in the differential diagnosis of {AD} and {FTLD}},
  year      = {2011},
  month     = {nov},
  number    = {23},
  pages     = {2034--2042},
  volume    = {77},
  doi       = {10.1212/wnl.0b013e31823b9c5e},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
}

@InProceedings{Singh2017,
  author    = {Shibani Singh and Anant Srivastava and Liang Mi and Kewei Chen and Yalin Wang and Richard J. Caselli and Dhruman Goradia and Eric M. Reiman},
  booktitle = {13th International Conference on Medical Information Processing and Analysis},
  title     = {Deep-learning-based classification of {FDG}-{PET} data for Alzheimer's disease categories},
  year      = {2017},
  editor    = {Jorge Brieva and Juan David Garc{\'{\i}}a and Natasha Lepore and Eduardo Romero},
  month     = {nov},
  publisher = {{SPIE}},
  doi       = {10.1117/12.2294537},
}

@Article{Rice2017,
  author    = {Rice, Louise and Bisdas, Sotirios},
  journal   = {European Journal of Radiology},
  title     = {The diagnostic value of {FDG} and amyloid {PET} in Alzheimer's disease---A systematic review},
  year      = {2017},
  month     = {sep},
  pages     = {16--24},
  volume    = {94},
  doi       = {10.1016/j.ejrad.2017.07.014},
  publisher = {Elsevier {BV}},
}

@Comment{jabref-meta: databaseType:bibtex;}
